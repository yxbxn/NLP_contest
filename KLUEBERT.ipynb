{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KLUEBERT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1rYIJ0jvGUHLpgPHtq-6Q2Z8QGSPyQgVM",
      "authorship_tag": "ABX9TyO0zWp/IfkcOGthGSqB/GMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yxbxn/NLP_contest/blob/main/KLUEBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)"
      ],
      "metadata": {
        "id": "QYl2ukp95Yxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "Nv5Dn6Xs5OXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "0Fig5P7w5OQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "76-6PQPk5OHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "uvkOESbG7YmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ],
      "metadata": {
        "id": "0GF6n1--6rcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4RChASn28La"
      },
      "outputs": [],
      "source": [
        "# 특정 파일 불러오기\n",
        "import pandas as pd\n",
        "#train_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/train_preprocessed.csv\")\n",
        "train_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/train_preprocessed_dup.csv\")\n",
        "test_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/test_preprocessed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tsv로 변환해보기"
      ],
      "metadata": {
        "id": "pimd2z5-LkHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preprocessed.head(3)"
      ],
      "metadata": {
        "id": "W5TzlRkrLuFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_preprocessed[[\"document\",\"label\"]].to_csv(\"train_preprocessed.tsv\",encoding=\"utf-8-sig\",index=False,sep=\"\\t\")"
      ],
      "metadata": {
        "id": "IyQi4vjCLjgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_seq_len = 64"
      ],
      "metadata": {
        "id": "a5OBIU7K3ayM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_preprocessed[\"label\"].value_counts()\n"
      ],
      "metadata": {
        "id": "-248Otqo43hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "label 데이터 인코딩하기"
      ],
      "metadata": {
        "id": "wvZLAHhL8ZA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_preprocessed['encoded_cat'] = train_preprocessed['label'].astype('category').cat.codes"
      ],
      "metadata": {
        "id": "aFjw81mO8Yu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#train_data,valid_data = train_test_split(train_preprocessed,test_size=0.2,shuffle=True)"
      ],
      "metadata": {
        "id": "9S-4S7li6LIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래에서 valid split 하기 때문에 split 할 필요는 없음\n",
        "import sklearn\n",
        "train_preprocessed = sklearn.utils.shuffle(train_preprocessed)"
      ],
      "metadata": {
        "id": "Ef6EuqqD6guN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label count 하기"
      ],
      "metadata": {
        "id": "Lbd1nOG34gMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
        "print('검증용 리뷰 개수 :',len(valid_data)) # 테스트용 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "zOhiqGqy3Dea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null 값 존재하는지 확인하기"
      ],
      "metadata": {
        "id": "qxDtnVL34rBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "#train_data = train_data.reset_index(drop=True)\n",
        "#print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "t-ehMWpO4kz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_data = test_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "#test_data = test_data.reset_index(drop=True)\n",
        "#print(test_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "xwHzLWKJ_VkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
        "print('테스트용 리뷰 개수 :',len(valid_data)) # 테스트용 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "dn9y9KiD3AO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KLUE 토크나이저"
      ],
      "metadata": {
        "id": "y7SyWWSo4xgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "#tokenizer = BertTokenizer.from_pretrained('klue/roberta-large')\n",
        "#tokenizer = BertTokenizer.from_pretrained('klue/roberta-small') # small로 학습시간 단축"
      ],
      "metadata": {
        "id": "bLf2yE4w4u56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(\"건물 내에서 고객의 요청에 따라 실내장식 공사 도배 등\"))\n",
        "print(tokenizer.encode(\"건물 내에서 고객의 요청에 따라 실내장식 공사 도배 등\"))"
      ],
      "metadata": {
        "id": "O9VXRbXU4kSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dqg6CLEcxGur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코딩된 것 다시 디코딩 가능함\n",
        "# 맨앞에 CLS 맨 뒤에 SEP\n",
        "print(tokenizer.decode(tokenizer.encode(\"건물 내에서 고객의 요청에 따라 실내장식 공사 도배 등\")))"
      ],
      "metadata": {
        "id": "bn9wcXxuA0bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_seq_len와 인코딩 결과"
      ],
      "metadata": {
        "id": "IuTiYXvY5r-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 64\n",
        "encoded_result = tokenizer.encode(\"건물 내에서 고객의 요청에 따라 실내장식 공사 도배 등\",max_length = max_seq_len,pad_to_max_length=True) # max_seq_len으로 인코딩\n",
        "print(encoded_result) #인코딩 결과 : 64짜리..."
      ],
      "metadata": {
        "id": "_YGme8bi5oJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 세그먼트 인풋\n",
        "print([0]*max_seq_len)"
      ],
      "metadata": {
        "id": "F2lM0PAoUVSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마스크 인풋 : 토큰이 있는 부분은 1 나머지 토큰이 없는 부분은 0\n",
        "valid_num = len(tokenizer.encode(\"건물 내에서 고객의 요청에 따라 실내장식 공사 도배 등\"))\n",
        "print(valid_num * [1] + (max_seq_len - valid_num) * [0])"
      ],
      "metadata": {
        "id": "Dt15l-qlyw6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 to feature"
      ],
      "metadata": {
        "id": "2zmDrWATzBuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
        "    \n",
        "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
        "    \n",
        "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
        "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
        "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
        "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
        "        token_type_id = [0] * max_seq_len\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(label)\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "\n",
        "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), data_labels"
      ],
      "metadata": {
        "id": "93jGtqQlyw4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.tail()"
      ],
      "metadata": {
        "id": "8YW86Xq4-EPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 64\n",
        "train_X, train_y = convert_examples_to_features(train_preprocessed['document'], train_preprocessed['encoded_cat'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
        "# tokenizer = BertTokenizer.from_pretrained('klue/roberta-large')"
      ],
      "metadata": {
        "id": "GqGT_VGDyw1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 철 cnc 절삭가공 치공구 부품 -> 인코딩 결과\n",
        "#tokenizer.decode(train_X[0][799996])"
      ],
      "metadata": {
        "id": "qGP_nkyB_7U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_X, test_y = convert_examples_to_features(valid_data['document'], valid_data['encoded_cat'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Q4JfwFAk7o_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대길이 64\n",
        "\n",
        "input_id = train_X[0][0]\n",
        "attention_mask = train_X[1][0]\n",
        "token_type_id = train_X[2][0]\n",
        "label = train_y[0]\n",
        "\n",
        "print('단어에 대한 정수 인코딩 :',input_id)\n",
        "print('어텐션 마스크 :',attention_mask)\n",
        "print('세그먼트 인코딩 :',token_type_id)\n",
        "print('각 인코딩의 길이 :', len(input_id))\n",
        "print('정수 인코딩 복원 :',tokenizer.decode(input_id))\n",
        "print('레이블 :',label)"
      ],
      "metadata": {
        "id": "RsJFNxByywnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer,AdamWeightDecay,TFRobertaModel,TFBertModel\n",
        "#model = TFRobertaModel.from_pretrained(\"klue/roberta-large\", from_pt=True)\n",
        "#model = TFRobertaModel.from_pretrained(\"klue/roberta-small\", from_pt=True)\n",
        "model = TFRobertaModel.from_pretrained(\"klue/roberta-base\", from_pt=True)"
      ],
      "metadata": {
        "id": "upFmik2jywhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사전학습된 TFBertModel KLUE 불러오기"
      ],
      "metadata": {
        "id": "eKTlHrXe5951"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 64"
      ],
      "metadata": {
        "id": "NaTSIegp5Jn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각각의 레이어 설정"
      ],
      "metadata": {
        "id": "V1sVdar20QY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
        "attention_masks_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
        "token_type_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
        "\n",
        "outputs = model([input_ids_layer, attention_masks_layer, token_type_ids_layer])"
      ],
      "metadata": {
        "id": "MAHwOXGv9b2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Klue_RobertaClassifier(tf.keras.Model):\n",
        "    def __init__(self, num_class):\n",
        "        super(Klue_RobertaClassifier, self).__init__()\n",
        "        #self.bert = TFRobertaModel.from_pretrained(\"klue/roberta-large\", from_pt=True)\n",
        "        #self.bert = TFRobertaModel.from_pretrained(\"klue/roberta-small\", from_pt=True)\n",
        "        self.bert = TFRobertaModel.from_pretrained(\"klue/roberta-base\", from_pt=True)\n",
        "        self.classifier = tf.keras.layers.Dense(num_class,\n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range,seed=42), \n",
        "                                                name='classifier')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask, token_type_ids = inputs\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        cls_token = outputs[1]\n",
        "        prediction = self.classifier(cls_token)\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "fWDDNer9VD56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Klue_RobertaClassifier(num_class=225)\n",
        "optimizer = AdamWeightDecay(1e-5,weight_decay_rate=1e-4)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics = [metric])"
      ],
      "metadata": {
        "id": "S-mwHSLePRiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding an ealrystop to prevent overfitting\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)"
      ],
      "metadata": {
        "id": "qzsLnhhgaelL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit & predict\n",
        "- max_seq_len이 64이기에 batch_size도 64로 설정\n",
        "- https://peltarion.com/knowledge-center/documentation/cheat-sheets/bert---text-classification-/-cheat-sheet"
      ],
      "metadata": {
        "id": "F2oTvZqVVaAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = F\"/content/MyDrive/MyDrive/test/\"\n",
        "#checkpoint_path = os.path.join(model_path,'weight_klue_robertav0_small.h5')\n",
        "checkpoint_path = os.path.join(model_path,'weight_klue_robertav0_base.h5')\n",
        "cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
      ],
      "metadata": {
        "id": "zEe8Rd_odQN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X, train_y, epochs=2, batch_size=64, validation_split=0.2, callbacks=[earlystop_callback, cp_callback]) "
      ],
      "metadata": {
        "id": "9Lr599-e9Vy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'weight_klue_robertav0_small.h5'\n",
        "path = F\"/content/MyDrive/MyDrive/test/{model_save_name}\" \n",
        "model.save_weights(path)"
      ],
      "metadata": {
        "id": "bse6jgBmbqPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_X, test_y, batch_size=1024)\n",
        "print(\"test loss, test acc: \", results)"
      ],
      "metadata": {
        "id": "069YuhGaVR4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MKNl4JClPYbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1dld6OHVQNsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}